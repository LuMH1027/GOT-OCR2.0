import json
from tqdm import tqdm
from transformers import AutoTokenizer
import os
# æŒ‡å®šæ¨¡å‹è·¯å¾„å’Œæ•°æ®è·¯å¾„
model_path = "/data_8t_1/qby/GOT-OCR2_0"
json_paths = [
    "/data_8t_1/dataset/tfr-dataset/image_and_json/MFR/data.json",
    "/data_8t_1/dataset/tfr-dataset/image_and_json/TR/data.json",
    "/data_8t_1/dataset/tfr-dataset/image_and_json/TFR/data.json"
]

# åŠ è½½ tokenizerï¼ˆå¿…é¡»åŒ…å«è‡ªå®šä¹‰ç‰¹æ®Š tokenï¼‰
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

# è·å– im token IDï¼ˆç¡®ä¿ä¸æ¨¡å‹ä¸€è‡´ï¼‰
im_start_token = 151857
im_end_token = 151858
# éå†æ¯ä¸ª json è·¯å¾„è¿›è¡Œæ¸…æ´—
for path in json_paths:
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)

    clean_data = []
    bad_data = []

    print(f"\nğŸ§¹ Cleaning: {path}")
    for sample in tqdm(data):
        try:
            text = sample["conversations"][1]["value"]
            input_ids = tokenizer(text).input_ids

            if input_ids.count(im_start_token) != input_ids.count(im_end_token):
                bad_data.append(sample)
            else:
                clean_data.append(sample)
        except Exception as e:
            sample["error"] = str(e)
            bad_data.append(sample)

    # è¾“å‡º clean/bad æ•°æ®
    base_dir = os.path.dirname(path)
    with open(os.path.join(base_dir, "clean_data.json"), "w", encoding="utf-8") as f:
        json.dump(clean_data, f, ensure_ascii=False, indent=2)

    with open(os.path.join(base_dir, "bad_data.json"), "w", encoding="utf-8") as f:
        json.dump(bad_data, f, ensure_ascii=False, indent=2)

    print(
        f"âœ… Done: {len(clean_data)} valid | âŒ {len(bad_data)} invalid saved.")
